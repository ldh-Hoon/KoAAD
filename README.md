# ê³ ë„í™”ëœ ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ë¥¼ ìœ„í•œ deepvoice audio detection

> ì„¸ì¢…ëŒ€í•™êµ 2024 1í•™ê¸° ìº¡ìŠ¤í†¤ë””ìì¸A 13ì¡° í”„ë¡œì íŠ¸ ì½”ë“œìš© repositoryì…ë‹ˆë‹¤. 

## ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ì°¸ì—¬ì
ì„¸ì¢…ëŒ€í•™êµ ì „ìì •ë³´í†µì‹ ê³µí•™ê³¼

[ì„ë™í›ˆ](https://github.com/ldh-Hoon), [ì´ì¤€í˜¸](https://github.com/Lanvizu), [ë¬¸í•œì†¡](https://github.com/MHANSONG), [ê¹€ì§„í™˜](https://github.com/kjhwan0115), ìœ¤ë‚˜ê²½(ì¶”ê°€ í•„ìš”)

## ğŸ“† í”„ë¡œì íŠ¸ ê¸°ê°„ 

2024ë…„ 3ì›” ~ 2024ë…„ 6ì›”

![image](https://github.com/ldh-Hoon/ko_deepfake-whisper-features/assets/139981434/85611a01-1d06-4cd1-99ce-12980923763f)

## ğŸ” ê°œìš”
### | í•œêµ­ì–´ ì¤‘ì‹¬ì˜ audio anti-spoofing ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œë„
êµ­ë‚´ì—ì„œë„ ë‹¤ì–‘í•œ ë”¥ë³´ì´ìŠ¤ í”¼ì‹± [ì‚¬ë¡€](https://imnews.imbc.com/replay/2024/nwtoday/article/6598469_36523.html)
ê°€ ë‚˜íƒ€ë‚˜ë©° ìƒˆë¡œìš´ ë°©ì‹ì˜ ë²”ì£„ê°€ í˜„ì‹¤í™”ë˜ê³  ìˆë‹¤.

í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ê³µê°œ ë°ì´í„°ëŠ” ì˜ì–´, ì¤‘êµ­ì–´ ë“± ì™¸êµ­ì–´ì— í•œì •ë˜ì–´ ìˆë‹¤. 

ë”°ë¼ì„œ AIHUB ë° ê³µê°œëœ TTS ìŒì„± ìˆ˜ì§‘ ë° ì˜¤í”ˆì†ŒìŠ¤ TTS ì‹œìŠ¤í…œì„ í™œìš©í•´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•´ë³´ê³ ì í–ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ ê°€ì§œ ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ì¸ KoAAD(Korean Audio Anti-spoofing Dataset)ë¥¼ ìˆ˜ì§‘ ë° ìƒì„±í–ˆë‹¤.

real audioëŠ” [AIHUB ê°ì„± ë° ë°œí™”ìŠ¤íƒ€ì¼ ë™ì‹œê³ ë ¤ ìŒì„±í•©ì„± ë°ì´í„°ì…‹](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=71349)ì„ ì‚¬ìš©í•œë‹¤.

í•´ë‹¹ ë°ì´í„°ì…‹ì˜ ìŒì„± ì „ì‚¬ label textë¥¼ ì‚¬ìš©í•´ í•©ì„±ëœ ìŒì„±ì„ ìƒì„±í•œë‹¤.

>Google Cloud TTS
 
>Melo TTS

>XTTS2

>Audio Pub 

>ë„¤ì´ë²„ ê¸°ì‚¬ ìŒì„± ìš”ì•½

>í´ë¡œë°”ë”ë¹™

ìµœì¢…ì ìœ¼ë¡œ ìœ„ 6ê°œì˜ ì‹œìŠ¤í…œì— ëŒ€í•´ ì•½ 10GB, 3ë§Œê°œ ë‚´ì™¸ì˜ ìŒì„± íŒŒì¼ì„ ìˆ˜ì§‘/ìƒì„±í•  ìˆ˜ ìˆì—ˆë‹¤.

### | ì‹¤ì œ í†µí™”ì™€ ìœ ì‚¬í•œ í™˜ê²½ì—ì„œì˜ íƒì§€ ë°ëª¨ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
ìŒì„±ë§Œ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì…‹ ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ê³¼ ë‹¬ë¦¬, ì‹¤ì œ ëŒ€ë¶€ë¶„ì˜ ì˜¤ë””ì˜¤ ì¡ìŒ ë° ì£¼ë³€ í™˜ê²½ìŒì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆë‹¤. 

ë˜í•œ í†µí™”ëŠ” ìµœì†Œ ë‘ëª…ì˜ í™”ìê°€ ëŒ€í™”í•˜ê²Œ ë˜ë¯€ë¡œ ì„±ëŠ¥ì— ì˜í–¥ì´ ìˆì„ ìˆ˜ ìˆë‹¤. 

ë”°ë¼ì„œ ë°ì´í„° ì¦ê°•ì„ í†µí•œ í•™ìŠµ, í†µí™”ë…¹ìŒ ë°ì´í„° ì‚¬ìš©, í™”ì ë¶„ë¦¬ ë“±ì„ ì ìš©í•˜ì—¬ ì‹¤ì œ í†µí™”ì— ëŒ€í•œ íƒì§€ ë°ëª¨ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³ ì ì‹œë„í–ˆë‹¤.

ì›ë³¸ repositoryì˜ whisper-LCNNì„ baseline ëª¨ë¸ë¡œ í™œìš©í–ˆë‹¤.

## demo ì˜ìƒ

[ìœ íŠœë¸Œ](https://youtu.be/BfzrxwfoHds)

## ê¸°íƒ€ ìë£Œ

[KoAAD ë° Colab ì½”ë“œ ìë£Œ](https://drive.google.com/drive/folders/1RihDtMWUc5sPg5fNqJ_L015U6HMgwF2u?usp=drive_link)
> ì•„ì§ ì •ë¦¬ê°€ ì˜ ì•ˆ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

train Colab: <a href="https://colab.research.google.com/drive/1RHPZg6mdu_0X6-DfjZDKjZHUu5LJWElN?usp=sharing"><img src="https://img.shields.io/badge/open in Colab-F9AB00?style=flat&logo=Google Colab&logoColor=white" /></a>

demo Colab: <a href="https://colab.research.google.com/drive/1Xph13KuqHoydh7Blj7qN-ccuROHHmi2d?usp=sharing"><img src="https://img.shields.io/badge/open in Colab-F9AB00?style=flat&logo=Google Colab&logoColor=white" /></a>

ë‹¤êµ­ì–´(í•œêµ­ì–´ ë¯¸í¬í•¨) fake audio ë°ì´í„°ì…‹ì¸ [MLAAD](https://owncloud.fraunhofer.de/index.php/s/tL2Y1FKrWiX4ZtP) ì™€ real audio ë°ì´í„°ì…‹ì¸ [MAILABS](https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/) ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì¢‹ë‹¤.



# ì°¸ê³  ë…¼ë¬¸

Kawa, Piotr, et al. "Improved DeepFake Detection Using Whisper Features." arXiv preprint arXiv:2306.01428 (2023).
[paper](https://arxiv.org/abs/2306.01428)

MÃ¼ller, Nicolas M., et al. "MLAAD: The Multi-Language Audio Anti-Spoofing Dataset." arXiv preprint arXiv:2401.09512 (2024).
[paper](https://arxiv.org/abs/2401.09512)




# ì•„ë˜ëŠ” ê¸°ì¡´ repositoryì˜ README.mdì…ë‹ˆë‹¤.





# Improved DeepFake Detection Using Whisper Features

The following repository contains code for our paper called "Improved DeepFake Detection Using Whisper Features".

The paper is available [here](https://www.isca-speech.org/archive/interspeech_2023/kawa23b_interspeech.html).


## Before you start

### Whisper
To download Whisper encoder used in training run `download_whisper.py`.

### Datasets

Download appropriate datasets:
* [ASVspoof2021 DF subset](https://zenodo.org/record/4835108) (**Please note:** we use this [keys&metadata file](https://www.asvspoof.org/resources/DF-keys-stage-1.tar.gz), directory structure is explained [here](https://github.com/piotrkawa/deepfake-whisper-features/issues/7#issuecomment-1830109945)),
* [In-The-Wild dataset](https://deepfake-demo.aisec.fraunhofer.de/in_the_wild).



### Dependencies
Install required dependencies using (we assume you're using conda and the target env is active):
```bash
bash install.sh
```

List of requirements:
```
python=3.8
pytorch==1.11.0
torchaudio==0.11
asteroid-filterbanks==0.4.0
librosa==0.9.2
openai whisper (git+https://github.com/openai/whisper.git@7858aa9c08d98f75575035ecd6481f462d66ca27)
```

### Supported models

The following list concerns models and its names to select it supported by this repository:
* SpecRNet - `specrnet`,
* (Whisper) SpecRNet - `whisper_specrnet`,
* (Whisper + LFCC/MFCC) SpecRNet - `whisper_frontend_specrnet`,
* LCNN - `lcnn`,
* (Whisper) LCNN - `whisper_lcnn`,
* (Whisper + LFCC/MFCC) LCNN -`whisper_frontend_lcnn`,
* MesoNet - `mesonet`,
* (Whisper) MesoNet - `whisper_mesonet`,
* (Whisper + LFCC/MFCC) MesoNet - `whisper_frontend_mesonet`,
* RawNet3 - `rawnet3`.

To select appropriate front-end please specify it in the config file.

### Pretrained models

All models reported in paper are available [here](https://drive.google.com/drive/folders/1YWMC64MW4HjGUX1fnBaMkMIGgAJde9Ch?usp=sharing).

### Configs

Both training and evaluation scripts are configured with the use of CLI and `.yaml` configuration files.
e.g.:
```yaml
data:
  seed: 42

checkpoint: 
  path: "trained_models/lcnn/ckpt.pth",

model:
  name: "lcnn"
  parameters:
    input_channels: 1
    frontend_algorithm: ["lfcc"]
  optimizer:
    lr: 0.0001
    weight_decay: 0.0001
```

Other example configs are available under `configs/training/`.

## Full train and test pipeline 

To perform full pipeline of training and testing please use `train_and_test.py` script.

```
usage: train_and_test.py [-h] [--asv_path ASV_PATH] [--in_the_wild_path IN_THE_WILD_PATH] [--config CONFIG] [--train_amount TRAIN_AMOUNT] [--test_amount TEST_AMOUNT] [--batch_size BATCH_SIZE] [--epochs EPOCHS] [--ckpt CKPT] [--cpu]

Arguments: 
    --asv_path          Path to the ASVSpoof2021 DF root dir
    --in_the_wild_path  Path to the In-The-Wild root dir
    --config            Path to the config file
    --train_amount      Number of samples to train on (default: 100000)
    --valid_amount      Number of samples to validate on (default: 25000)
    --test_amount       Number of samples to test on (default: None - all)
    --batch_size        Batch size (default: 8)
    --epochs            Number of epochs (default: 10)
    --ckpt              Path to saved models (default: 'trained_models')
    --cpu               Force using CPU
```

e.g.:
```bash
python train_and_test.py --asv_path ../datasets/deep_fakes/ASVspoof2021/DF --in_the_wild_path ../datasets/release_in_the_wild --config configs/training/whisper_specrnet.yaml --batch_size 8 --epochs 10 --train_amount 100000 --valid_amount 25000
```


## Finetune and test pipeline

To perform finetuning as presented in paper please use `train_and_test.py` script.

e.g.:
```
python train_and_test.py --asv_path ../datasets/deep_fakes/ASVspoof2021/DF --in_the_wild_path ../datasets/release_in_the_wild --config configs/finetuning/whisper_specrnet.yaml --batch_size 8 --epochs 5  --train_amount 100000 --valid_amount 25000
```
Please remember about decreasing the learning rate!


## Other scripts

To use separate scripts for training and evaluation please refer to respectively `train_models.py` and `evaluate_models.py`.


## Acknowledgments

We base our codebase on [Attack Agnostic Dataset repo](https://github.com/piotrkawa/attack-agnostic-dataset).
Apart from the dependencies mentioned in Attack Agnostic Dataset repository we also include: 
* [RawNet3 implementation](https://github.com/Jungjee/RawNet).



## Citation

If you use this code in your research please use the following citation:
```
@inproceedings{kawa23b_interspeech,
  author={Piotr Kawa and Marcin Plata and MichaÅ‚ Czuba and Piotr SzymaÅ„ski and Piotr Syga},
  title={{Improved DeepFake Detection Using Whisper Features}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={4009--4013},
  doi={10.21437/Interspeech.2023-1537}
}
```
